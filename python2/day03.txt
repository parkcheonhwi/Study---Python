

파일 입출력(csv) -> 웹 클롤링

1. 파일입출력(csv파일, Comma Separated Value)
	데이터를 콤마(,)로 구분한 형식
	스프레드 시트 or 데이터베이스에서 자주 이용

	import csv
	writer(파일): 라이터 인스턴스를 얻음
	reader(파일): 리더 인스턴스를 얻음
	라이터,writerow(시퀀스): csv 파일에 1행으로 써넣음
	라이터.writerows(시퀀스) : csv파일에 여러행으로 써넣음

2. 웹 크롤링

1) 개념
	크롤링: 여러 웹 페이지를 기계적으로 탐색하는 것
	스크래핑: 특정한 하나의 웹 페이지를 탐색하고 소스코드 작성자가 원하는 정보를 얻어내는 것
	=> 완성된 웹 페이지에서 필요한 정보를 수집하여 추출하는 과정

크롤링시 주의사항
	무분별한 웹 크롤링은 웹서버에 과부화 문제를 유발, 크롤링하기 전 robots.txt 파일 확인

2) 웹 크롤링 방법
	사용자는 브라우저로 접속하고 싶은 주소 입력
	브라우저가 해당 주소의 서버에게 정보를 달라고 요청
	웹 서버는 구성에 필요한 정보를 코드형태(html)로 전달
	브라우저는 전달받은 코드를 해석 -> 사용자 화면에 보여줌

	순서1) 웹 페이지 정보를 가져온다(requests 라이브러리 사용)
	순서2) html 코드를 파싱(분석)하여 원하는 정보 습득(beautifulSoup 라이브러리 사용)

3. 웹 크롤링 하기 전 알아야하는 사전 지식
1) 웹/ 인터넷

-웹 (web)

② 인터넷(Internet)
   물리적으로 떨어져있는 서버와 서버를 연결해주는 것
   TCP/IP 통신 프로토콜을 이용해 정보를 주고받는 컴퓨터 네트워크
   TCP : 서버와 클라이언트 간에 데이터를 신뢰성있게 전달하기 위해 만들어진 프로토콜
   IP : Internet Protocol의 줄임말

③ URL(Uniform Resource Locator)
   어떤 자원의 위치를 표기하는 방법을 의미
   ex) www.naver.com

   => https://www.naver.com
   프로토콜://도메인:포트번호/경로
   
   http, https => 하이퍼텍스트 문서를 전송할 때 사용하는 프로토콜(통신규약-약속)
   도메인 => 사이트명
   포트번호 => 기본포트 80, 443(생략)
    
   https://search.naver.com/search.naver?   where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=python

   요청파라미터   값
   sm      top_hty
   fbm      0
   ie      utf8
   query      python

2) 웹 서버와 웹 클라이언트
① 웹 서버(Web Server)
   클라이언트언의 요청에 따라 웹페이지, 이미지, 동영상 등을 클라이언트에게 제공
   
② 웹 클라이언트(Web Client)
   사용자가 웹 브라우저를 통해 웹 페이지를 요청하고 웹 서버로부터 받아온 정보를
   표시하는 소프트웨어
   
요청(request) : 웹 클라이언트가 웹서버에 필요한 정보를 요구하거나 처리해달라고 부탁하는 행위
      웹에서 발생하는 요청 HTTP요청
응답(response) : 웹 클라이언트 요청을 수신받고 필요한 데이터를 전달하는 행위
      웹에서 발생하는 응답 HTTP 응답

4) 모듈, 패키지, 라이브러리
모듈
   구성단위
   전역변수 및 함수 등을 모아둔 파일

패키지
   특정기능과 관련된 여러가지 모듈들의 집합(폴더)

라이브러리
   모듈과 패키지가 합쳐진 것을 의미함
   기본 라이브러리와 외부 라이브러리로 나뉨

4. 웹 크롤링 패키지
1) requests : 파이썬에서 동작하는 작고 빠른 브라우저
   웹 사이트에 요청을 쉽게 처리하는 라이브러리
   HTML 문서를 가져올 때 사용하는 패키지
   python에서 HTTP 통신을 할 때 가장 자주 쓰이는 패키지로 열기과정에서 사용가능

사용방법
   vs code : 터미널에서 pip install requests 설치 후
      import requests
   colab : 설치할 필요없이 맨 윗줄에 import requests

requests로 웹 페이지 가져오기
   import requtests
   url = "url주소"
   response = requests.get(url)
   
requests 메소드
   .status_code : 상태코드 확인(1xx: 진행중, 2xx : 완료, 3xx: 완료인데 페이지이동,
            4xx : 사용자잘못, 5xx : 서버오류)
   .encoding : 언어설정
   .text : 웹페이지 소스 우리 눈에 보기 쉬운 형태
   .content : 웹페이지 소스 모든 문자 그대로

2) BeautifulSoup4 : HTML parser
   구문을 해석해서 필요한 내용만 추출하는 패키지

사용방법
   import requests
   from bs4 import BeautifulSoup as bs
   
   url = "url주소"
   response = requtests.get(url)
   html = response.text
   soup = bs(html, 'html.parser')

메소드
   .find('tagname') : 태그 중에서 태그명이 tagname인 첫번째 것
   .find('tagname').text : 태그는 제외하고 내용만 추출
   .find('tagname').get('property') 
      : 태그명이 tagname인 것 중 속성명이 property인 첫번째 것
   .find_all('tagname') : 태그 중에서 태그명이 tagname인 것들의 리스트
      .find_all('tagname')[1].text

5. 헤더(header)

5. 헤더(header)
1) User-Agent : 사용자 소프트웨어 식별 정보
   내가 직접 접속하는 것과 requests를 통해 접속하는 user-agent가 다르기 때문에
   크롤링이 안될 수 있음(406에러)

2) 필요성
   무분별한 크롤링과 서버 과부화를 막기 위해 프로그램을 통하여 접속하는 것을 차단하는 사이트
   내가 원하는 정보를 추출할 수 없는 경우가 발생하는데 그 때 헤더를 사용

3) 내 User-Agent 확인 사이트
   - https://www.useragentstring.com/
      브라우저 설정해서 가져올 수 있음
   - https://www.whatismybrowser.com/detect/what-is-my-user-agent/   
      현재 운영체제의 User-Agent 가져올 수 있음

Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36
   
4) 사용법
   header = {"User-Agent" :"나의 User-Agent(복붙)"}
   response = requests.get(url, headers = header)


6. 데이터 분석 패키지
   넘파이(numpy)          수치해석을 위한 라이브러리
   판다스(pandas)
   맷플로립(matplotlib)
   그 외 : math, stastmodels, ...

vs code 터미널 pip install numpy
      pip install pandas

vs code, colab
   import numpy as np
   import pandas as pd


1) list 구조
   순서를 가지고 일렬로 나열한 원소들의 모임
   크기가 변할 수 있는 동적할당
   [1, 0.5, 'str', True] 모든 타입을 보존함(여러가지 자료형을 허용함)
   2차원 이상의 배열 구조일 경우 내부 배열에서 원소의 개수가 달라도 됨
   ex) [[1,2], [1, 2, 3]]
   list + list => 연결, list * int => 반복

2) 넘파이 배열 구조
   다차원 배열인 ndarray 클래스의 객체, 하나의 자료형으로만 만들어진 원소들의 보관
   고정된 크기를 갖는 정적할당
   내부 배열 내 원소 개수는 모두 같아야함
   ex) [[1,2], [1, 2]]
   넘파이배열 + 넘파이배열 => 같은 자리 원소끼리 연산(더해짐)


7. 넘파이(Numerical Python)
   수치 연산을 빠르게 할 수 있는 수치 해석을 위한 라이브러리
   배열을 표현하기 위한 패키지(숫자로 된 큰 배열 데이터를 다룰 때 사용)
   데이터 사이언스, 빅데이터 분석, 머신러닝, 딥러닝 진행 시 필수적임
   넘파이 공식문서
   https://numpy.org/doc/stable/

1) 특징
   컴퓨터 과학을 위한 기본적인 파이썬 패키지
   n차원의 데이터를 빠르게 처리
   c, c++, 포트란 만들어짐
   선형대수학을 빠르게 계산

2) 넘파이 객체의 속성
   .shape : 배열의 각 차원의 크기, 튜플 형태 표현
   (ex) n행 m열의 행렬인 경우 (n, m)형태로 출력
   3x5 => 3행 5열 => (3, 5)

   .ndim : 배열의 차원수 or 배열의 축의 개수
   .dtype : 배열의 각 요소 타입
   .size : 전체 요소의 개수

3) 넘파이 추가, 수정, 검색, 삭제
   추가    객체 = np.append(배열, 추가할 값)
   수정   배열[인덱스] = 값
   검색    배열[인덱스]
      np.where(인덱스) 특정 값을 찾을 수 있음
   삭제   객체 = np.delete(배열, 삭제할 값)
